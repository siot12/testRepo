{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZPpDmTwt3zRxdEdDpaSdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siot12/testRepo/blob/main/Good_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache SpamAssassin Project maintains a nice collection of old e-mail messages that we can use. The messages age from early 2000’s, and probably the scammers are way smarter now, so please don’t use this in any production environment :)\n",
        "\n",
        "Download the data set using the following code:"
      ],
      "metadata": {
        "id": "nCXkzMy-1akP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import makedirs, path, remove, rename, rmdir\n",
        "from tarfile import open as open_tar\n",
        "from urllib import request, parse\n",
        "\n",
        "\n",
        "def download_corpus(dataset_dir: str = 'data2'):\n",
        "    base_url = 'https://spamassassin.apache.org'\n",
        "    corpus_path = 'old/publiccorpus'\n",
        "    files = {\n",
        "        '20021010_easy_ham.tar.bz2': 'ham',\n",
        "        '20021010_spam.tar.bz2': 'spam',\n",
        "    }\n",
        "\n",
        "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
        "    ham_dir = path.join(dataset_dir, 'ham')\n",
        "    spam_dir = path.join(dataset_dir, 'spam')\n",
        "\n",
        "\n",
        "    makedirs(downloads_dir, exist_ok=True)\n",
        "    makedirs(ham_dir, exist_ok=True)\n",
        "    makedirs(spam_dir, exist_ok=True)\n",
        "\n",
        "    for file, spam_or_ham in files.items():\n",
        "        # download file\n",
        "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
        "        tar_filename = path.join(downloads_dir, file)\n",
        "        request.urlretrieve(url, tar_filename)\n",
        "\n",
        "        # list e-mails in compressed file\n",
        "        emails = []\n",
        "        with open_tar(tar_filename) as tar:\n",
        "            tar.extractall(path=downloads_dir)\n",
        "            for tarinfo in tar:\n",
        "                if len(tarinfo.name.split('/')) > 1:\n",
        "                    emails.append(tarinfo.name)\n",
        "\n",
        "        # move e-mails to ham or spam dir\n",
        "        for email in emails:\n",
        "            directory, filename = email.split('/')\n",
        "            directory = path.join(downloads_dir, directory)\n",
        "            rename(path.join(directory, filename),\n",
        "                   path.join(dataset_dir, spam_or_ham, filename))\n",
        "\n",
        "        rmdir(directory)\n",
        "\n",
        "\n",
        "download_corpus()"
      ],
      "metadata": {
        "id": "sRUhLP3F1ilV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a corpus of 6952 hams and 2399 spams."
      ],
      "metadata": {
        "id": "ypPRB37A2I_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from os import path\n",
        "ham_dir = path.join('data2', 'ham')\n",
        "spam_dir = path.join('data2', 'spam')\n",
        "print('hams:', len(glob(f'{ham_dir}/*')))  # hams: 6952\n",
        "print('spams:', len(glob(f'{spam_dir}/*')))  # spams: 2399"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4G57TcdL2LPu",
        "outputId": "529f95fa-f655-4256-b298-45516634092f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hams: 2551\n",
            "spams: 501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing messages**\n",
        "\n",
        "If you open any of these individual files, you will see they are very hard to read. This is because they are in MIME format. Python has a standard library that helps us to extract only the part that we care about, namely subject and body.\n",
        "\n",
        "Let’s create a class to represent a message. This class will hold the subject, the body, and will have a method to retrieve a clean string containing only letters."
      ],
      "metadata": {
        "id": "R47mtq7A2Wul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import sub\n",
        "class SimpleEmail:\n",
        "    def __init__(self, subject: str, body: str):\n",
        "        self.subject = subject\n",
        "        self.body = body\n",
        "\n",
        "    @property\n",
        "    def clean(self):\n",
        "        sanitizer = '[^A-Za-z]+'\n",
        "        clean = sub(sanitizer, ' ', f'{self.subject} {self.body}')\n",
        "        clean = clean.lower()\n",
        "        return sub('\\s+', ' ', clean)\n",
        "\n",
        "    def __str__(self):\n",
        "        subject = f'subject: {self.subject}'\n",
        "        body_first_line = self.body.split('\\n')[0]\n",
        "        body = f'body: {body_first_line}...'\n",
        "        return f'{subject}\\n{body}'\n",
        "    def __repr__(self):\n",
        "        return self.__str__()"
      ],
      "metadata": {
        "id": "7D64C2tT2er0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we first started, we thought these messages would be heavy to load at once in memory, and because of that we built this generator for reading messages from a directory. In the end the messages are not that heavy…"
      ],
      "metadata": {
        "id": "tOMNGd2I2qH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from email import message_from_file\n",
        "from glob import glob\n",
        "class EmailIterator:\n",
        "    def __init__(self, directory: str):\n",
        "        self._files = glob(f'{directory}/*')\n",
        "        self._pos = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._pos = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._pos < len(self._files) - 1:\n",
        "            self._pos += 1\n",
        "            return self.parse_email(self._files[self._pos])\n",
        "        raise StopIteration()\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_email(filename: str) -> SimpleEmail:\n",
        "        with open(filename,\n",
        "                  encoding='utf-8',\n",
        "                  errors='replace') as fp:\n",
        "            message = message_from_file(fp)\n",
        "\n",
        "        subject = None\n",
        "        for item in message.raw_items():\n",
        "            if item[0] == 'Subject':\n",
        "                subject = item[1]\n",
        "\n",
        "        if message.is_multipart():\n",
        "            body = []\n",
        "            for b in message.get_payload():\n",
        "                body.append(str(b))\n",
        "            body = '\\n'.join(body)\n",
        "        else:\n",
        "            body = message.get_payload()\n",
        "\n",
        "        return SimpleEmail(subject, body)"
      ],
      "metadata": {
        "id": "UQwkb1j82t9f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**\n",
        "\n",
        "Let’s load everything in memory, we will be fine"
      ],
      "metadata": {
        "id": "Kk8QxP7S2x4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ham_emails = EmailIterator('data2/ham')\n",
        "spam_emails = EmailIterator('data2/spam')\n",
        "hams = np.array([email.clean for email in ham_emails])\n",
        "spams = np.array([email.clean for email in spam_emails])"
      ],
      "metadata": {
        "id": "5Nw-82la2xmF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have an unbalanced data set (6952 hams and 2399 spams) we need to take care when splitting it in training and test sets. We can use Scikit-Learn’s StratifiedShuffleSplit for that. It will make sure that we have a homogeneous distribution of hams and spams in both training and test sets."
      ],
      "metadata": {
        "id": "swgrerEu274d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "emails = np.concatenate((hams, spams))\n",
        "labels = np.concatenate((np.zeros(hams.size), np.ones(spams.size)))\n",
        "for train_index, test_index in split.split(emails, labels):\n",
        "    emails_train, labels_train = \\\n",
        "        emails[train_index], labels[train_index]\n",
        "    emails_test, labels_test = \\\n",
        "        emails[test_index], labels[test_index]"
      ],
      "metadata": {
        "id": "oABat9js29An"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the messages in the training set, we build a dictionary with the occurrences of each word across all messages."
      ],
      "metadata": {
        "id": "lb7zgbzO3Awn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "dictionary = defaultdict(int)\n",
        "for email in emails_train:\n",
        "    for word in email.split(' '):\n",
        "        dictionary[word] += 1"
      ],
      "metadata": {
        "id": "rS_O2CCK3Bex"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we select only the top 1000 most frequent ones (you can experiment varying this number). Also, notice we are ignoring single letters (len(word) > 1)."
      ],
      "metadata": {
        "id": "b-cu-Evf3Dho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top = 1000\n",
        "descending_dictionary = sorted(dictionary.items(),\n",
        "                               key=lambda v: v[1],\n",
        "                               reverse=True)\n",
        "dictionary = [\n",
        "    word for (word, occur) in descending_dictionary\n",
        "    if len(word) > 1\n",
        "][:top]"
      ],
      "metadata": {
        "id": "X0OtXwUr3Fop"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea now is that we will encode each message (subject + body) into an array where each index indicates how many times a given word appears there. For instance, if our dictionary was only:"
      ],
      "metadata": {
        "id": "3T1Lk3323JW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\"yes\", \"no\", \"have\", \"write\", \"script\", \"myself\", \"to\"]"
      ],
      "metadata": {
        "id": "l40Ad6bLkltb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And a certain message is “I would prefer not to have to write a script myself”, it would be encoded as:"
      ],
      "metadata": {
        "id": "Cvj4DsAykmd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[0, 0, 1, 1, 1, 1, 2]"
      ],
      "metadata": {
        "id": "e-7Yg4HWkoyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which means:"
      ],
      "metadata": {
        "id": "3XxkBm-6kqk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    0,  # zero occurrence(s) of word yes\n",
        "    0,  # zero occurrence(s) of word no\n",
        "    1,  # one  occurrence(s) of word have\n",
        "    1,  # one  occurrence(s) of word write\n",
        "    1,  # one  occurrence(s) of word script\n",
        "    1,  # one  occurrence(s) of word myself\n",
        "    2   # two  occurrence(s) of word to\n",
        "]"
      ],
      "metadata": {
        "id": "7d3QjgpBksUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could also do “0” or “1” for “not occur” or “occur”. The following function encodes a given message using the approach just described."
      ],
      "metadata": {
        "id": "4kb0jlr9k8P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_email(email: SimpleEmail,\n",
        "                 dictionary_: list,\n",
        "                 binary: bool = False) -> np.array:\n",
        "    encoded = np.zeros(dictionary_.size)\n",
        "    words = email.split(' ')\n",
        "\n",
        "    for word in words:\n",
        "        index = np.where(dictionary_ == word)[0]\n",
        "        if index.size == 1:  # we ignore unknown words\n",
        "            if binary:\n",
        "                encoded[index[0]] = 1\n",
        "            else:\n",
        "                encoded[index[0]] += 1\n",
        "    return encoded\n"
      ],
      "metadata": {
        "id": "oJCIdmrFk9yX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we encode our messages."
      ],
      "metadata": {
        "id": "OAkrfNgTu4Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "dictionary = np.array(dictionary)\n",
        "_encode_email = partial(encode_email, dictionary_=dictionary)\n",
        "encoded_train = np.array(list(map(_encode_email, emails_train)))\n",
        "encoded_test = np.array(list(map(_encode_email, emails_test)))\n",
        "print(encoded_train);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xWv0E50auvFS",
        "outputId": "f5a95699-b04f-4424-facb-e37e8e615ced"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.  2.  2. ...  0.  0.  0.]\n",
            " [ 3.  2.  1. ...  0.  0.  0.]\n",
            " [ 5.  2.  2. ...  0.  0.  0.]\n",
            " ...\n",
            " [13.  5.  4. ...  0.  0.  0.]\n",
            " [12. 13.  6. ...  0.  0.  0.]\n",
            " [10. 12.  9. ...  0.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron training**"
      ],
      "metadata": {
        "id": "pghWsvecwOC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import numpy as np\n",
        "\n",
        "perceptron = Perceptron(max_iter=5, random_state=45)\n",
        "#perceptron.fit(encoded_train, labels_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "#labels_pred = perceptron.predict(encoded_train)\n",
        "labels_pred = cross_val_predict(perceptron,encoded_train,labels_train,cv=5)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(labels_train, labels_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "print('precision:', precision_score(labels_train, labels_pred))\n",
        "\n",
        "print('recall:', recall_score(labels_train, labels_pred))\n",
        "\n",
        "print('f1:', f1_score(labels_train, labels_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6doXqlKwLpi",
        "outputId": "0f5e76d5-4323-416b-c7f1-4f66b5e21141"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.50%\n",
            "precision: 0.9521276595744681\n",
            "recall: 0.8927680798004988\n",
            "f1: 0.9214929214929216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron Prediction**"
      ],
      "metadata": {
        "id": "0ARSKJVY9MpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron.fit(encoded_train,labels_train)\n",
        "labels_pred = perceptron.predict(encoded_test)\n",
        "\n",
        "print('accuracy:', accuracy_score(labels_test, labels_pred))\n",
        "\n",
        "print('precision:', precision_score(labels_test, labels_pred))\n",
        "\n",
        "print('recall:', recall_score(labels_test, labels_pred))\n",
        "\n",
        "print('f1:', f1_score(labels_test, labels_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoXPRiK89Oo0",
        "outputId": "de39113a-6be0-4618-a83b-549f8db03a5e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8690671031096563\n",
            "precision: 0.5581395348837209\n",
            "recall: 0.96\n",
            "f1: 0.7058823529411763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}